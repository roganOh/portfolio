# [AIRFLOW]
## airflow란?

##### airflow는 airbnb에서 방대해져만 가는 자신들의 데이터들의 워크플로우를 해결&정리 하기 위해서 만든 오픈소스 툴이프로그램이다.
##### ETL툴은 아니며 워크플로우 툴이다. 대표적으로 머신러닝의 과정에서도 사용하기도 하지만 그 워크플로우가 잘 구성되어 있어 대부분 ETL 툴로 쓴다
##### 파이썬으로 구성되어 있으며 .py 파일로 구성되어 있다.

## 장점

##### 코드 구성원칙을 따른다.
     모든 설정,구성들이 모두 코드로 이루어 진다는 말이다. 즉 모든 기능은 UI에서 뿐만 아니라 코드만으로도 해결할 수 있다.
     이때문에 에어플로우에서 제공하는 기능들중 사용자의 맘에 드는 것이 없다면 사용자가 만들어서 쓸 수 있고, 에어플로우에선 그것을 위해 base가 되는 틀을 만들어놨다. ( baseoperator,basehook,baseplugin 등등..)
##### dag,task 간의 dependency를 설정하기 쉽다.(에어플로우의 기능과 구성요소등은 밑에서 설명하겠다.)
##### backfill 이 쉽다.
##### executiondate

## 단점

##### 코드 구성원칙을 따르며 너무 많은 기능들이 있어서 진입장벽이 높으며 비교적 learning curve 가 처참하다.
##### 개발 싸이클이 비교적 길다. 여기에는 짠 코드를 테스트해보는것이 어려운것이 보탬을 하기도 했다.
##### 파이썬 특성상, 코드 구성원칙의 특성상, 에어플로우의 특성상 유지보수가 매우 어렵다.
##### executiondate (장점이자 단점이다. 이게 뭔소린가 싶겠지만 밑에서 설명하겠다.)
##### 실시간으로 돌아갈 수 없다

## 설치방법


### 1. airflow를 설치한다
[install airflow 1.10.14](https://github.com/roganOh/portfolio/blob/master/chequer/querypie_el/install_airflow_11014_in_ec2.md)
### 2.airflow의 메타데이터가 저장될 위치를 정한다
```
cd 
cd airflow
vim airflow.cfg
    broker_url,result_backend,sql_alchemy_conn 를 고칩니다. db는 mysql 혹은 PG가 좋고, 같은 위치로 합니다. 단 sqla+mysql 이런 부분들 앞에서 +앞쪽은 고치지 맙시다.
    executor을 원하는 executor로 바꿉니다.
```

## 에어플로우의 핵심 구성 요소 세가지
##### 에어플로우엔 핵심 구성 요소로 DAG,TASK,SCHEDULER가 존재한다.
##### 에어플로우는 워크플로우툴임을 기억하고 있자.

## DAG

##### DAG는 Directed Acyclic Graph로 한국말로는 방향성 비순환 그래프이다.
##### DAG를 이루는 워크 하나하나를 TASK라고 부르는데 이것들은 서로 방향성을 가지고 있고, 실행순서에 순환이 없게 만들어야만 한다.
##### 하나의 워크플로우이다. 
##### 여러개 설정가능하며, 모두 각자의 스케쥴을 가지고 있다.
##### task들이 모여서 워크플로우가 된다.

## TASK

##### DAG를 이루는 일 하나하나를 TASK라고 부른다.
##### 모든 TASK는 operator과 일대일 관계를 가지며, 이 operator의 종류에 따라 task의 종류, task가 하는일이 정해진다.

## SCHEDULER


##### DAG 와 TASK를 스케쥴에 맞게 돌리는 역할을 한다.
    후에 서버 직렬화를 하고 싶다면 serialize 부문을 참고하자. 반드시 필요한 작업이다.  
    또한 backend를 sqlite가 아닌 메타데이터를 저장할 위치로 설정을 반드시 해야하며  
    시간이 된다면 2.0 이상의 버전을 사용하자. 속도가 몇백 배 빠르다.
    executor은 sequencial executor을 사용하지 말고 local executor을 사용하면서 병렬화 수를 설정해두자. 스케쥴러가 부족한 현상을 막을 수 있다.
    스케쥴러의 비서같은 일을 하는 worker라는 개념이 있는데 이것의 디폴트 값은 4이다. 컴퓨터 혹은 서버가 버벅거리면 worker을 줄이고, 서버는 상태가 괜찮음에도 불구하고, 속도가 느리면 worker를 늘리자. 
    'airflow' 명령어를 실행했을 때 반응속도가 늦으면 서버가 버벅거리는 것이다.
    airflow.cfg 를 고쳤는데도 설정이 바뀌지 않는 경우가 있다. 그에 대한 해결방법이다.
```
1. 서버와 스케쥴러를 모두 죽였다가 다시 시작하자. 다시 시작할 땐 db init또한 필요하다.

 db init을 할땐 주의하자. connection 이 다 사라진다. connection은 db에서 읽어오는 정보들이기 때문에 db init을 시켜버리면 다 날라간다. 반드시 백업을 만들어 두자.
 백업은 직접 테이블을 복사해두던지 아니면 에어플로우에서 제공하는 백업기능을 사용하자. (사실 믿을만한 기능은 아니다. 될 수 있으면 테이블을 쿼리를 통해 백업을 해두자.)
2. 그것도 안되면 airflow docs에 들어가면 각 설정값에 대한 횐경변수값들이 있다. 이들을 수동으로 설정해주자. 결국 마지막엔 환경변수값들을 읽어서 airflow에 반영하기 때문에 수동으로 설정해주면 만사 ok이다.
```
## airflow 를 사용한 이유
##### 근본적인 목적은 스티치를 대체하기 위해서이다. (stitch.data)
##### 스티치에는 여러 문제가 있었다.

```
1. 스티치는 saas이다. 
2. 10000 줄의 데이타를 옮기면 10000줄이 아닌 12000~20000줄에 해당하는 크레딧을 결제해야했다.(이유는 옮기는 row말고도 기타 row들이 추가로 생성되었기 때문에)
3. saas임에도 불구하고 데이타를 옮기면 칼럼들이 모두 abc순으로 나열 되어 기존 사용자가 설정해둔 칼럼순서와 완전히 맞지 않게 된다.
4. 데이타를 옮기면 사용자가 설정해둔 칼럼외에 4~5개의 칼럼이 새로 생성된다. 이들은 각각 스티치가 언제 데이터를 추출했는지, 언제 로드 했는지 등에 대한 데이터 들이다.
5. 스티치는 snowflake의 크레딧을 하루에 25 크레딧이상을 쓰게 했다.(1크레딧당 2~3달러)
6. 스티치의 고정적인 사용자가 되면 이전과 차원이 다른 꽤 많은 돈을 요구한다.
7. saas임에도 불구하고 머지는 사용제약이 상당히 많고 increasement 는 테이블의 칼럼이 바뀔 경우 고장나 계속 버그가 났다.
```
##### 이러한 스티치를 대체함으로 써 다음과 같은 것들을 해결 할 수 있었다.
```
1. 스티치로 인한 돈을 더이상 쓰지 않아도 된다.
2. snowflake에 데이터를 밀어 넣는데에 (querypie,sqlgate,jira,zendesk,ga등등..) 2021/1/21일 기준 하루에 1 크레딧을 사용한다. (±0.2)
3. 만약 데이터의 내용이나 칼럼이 바뀐다면 30분 안에 해결 할 수 있다.
4. dbt가 하는 일, ella가 하던 노가다들을 모두 자동화 시킬 수 있었고, dbt,superset 등과의 호완이 가능하다.
```
